# TiDB Lightning Configuration for VT Parquet Import
# Usage: tiup tidb-lightning --config lightning.toml

[lightning]
# Log level
level = "info"
# Log file
file = "tidb_lightning.log"
# Check requirements before import
check-requirements = true

[tikv-importer]
# Backend mode: "local" for high-speed bulk import
backend = "local"
# Directory for sorted KV data (requires ~2x source data size)
sorted-kv-dir = "/data/tidb_import/sorted"
# Parallel import workers
range-concurrency = 16

[mydumper]
# Directory containing Parquet files
data-source-dir = "/data/vt_export"
# File pattern to match
# Note: Lightning expects SQL/CSV by default, Parquet support is limited
# You may need to convert or use Dumpling format
no-schema = false
# Filter rules (glob patterns)
filter = ['*.parquet']

[tidb]
# TiDB connection
host = "tidb-dev-us-ashburn-1.cybereason.net"
port = 4000
user = "root"
password = "kDraCqVFihq5WwSh"
status-port = 10080
# PD address (required for local backend)
pd-addr = "pd-dev-us-ashburn-1.cybereason.net:2379"

[tidb.security]
# Disable TLS
# cluster-tls = false

[checkpoint]
# Enable checkpoint for resume capability
enable = true
driver = "file"
# Checkpoint file location
dsn = "/tmp/tidb_lightning_checkpoint.pb"

[post-restore]
# Run ANALYZE after import for better query performance
level1-compact = true
compact = true
analyze = "required"

# Table schema mapping (if Parquet schema doesn't match TiDB)
# Note: Lightning's Parquet support is limited, may need preprocessing
